{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=15\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"8:00\" #1 hour\n",
    "os.environ['SBATCH_PARTITION']='lattice' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName('dfTest').getOrCreate()\n",
    "df = spark.read.csv('Data/AirOnTimeCSV/airOT20*',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove empty last column and print schema\n",
    "df = df.select(df.columns[:44])\n",
    "#print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# limit to just flights out of Houston (IAH)\n",
    "df = df.where(df.ORIGIN == \"IAH\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only delayed flights\n",
    "I only did this preprocessing because it is what we talked about, but I don't think we should use this for training the model. See all arrived flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only delayed flights\n",
    "deldf = df.where(df.ARR_DELAY_NEW > 0)\n",
    "deldf.show(5)\n",
    "#print(deldf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and label\n",
    "deldf = deldf.select(\"YEAR\", \"MONTH\", \"DAY_OF_WEEK\", \"UNIQUE_CARRIER\", \\\n",
    "               \"DEST\", \"DEST_STATE_ABR\", \"CRS_DEP_TIME\",\\\n",
    "               \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"ARR_DELAY_NEW\")\n",
    "deldf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "print(deldf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only take last 5 years of data to reduce size of dataset so it can be processes on one machine\n",
    "deldf = deldf.where(deldf.YEAR > 2007)\n",
    "deldf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "print(deldf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features and label in files\n",
    "years = [2012-x for x in range (5)] # list of years in reverse.\n",
    "\n",
    "for year in years:\n",
    "        filename = \"FeaturesLabels\" + str(year) + \".csv\"\n",
    "        print(filename)\n",
    "        deldf.where((deldf.YEAR == year)).toPandas().to_csv(\"./Data/preprocR1-2/onlyDelayed/\"+filename, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Arrived Flights\n",
    "Using this data instead of just delayed flights allows us to check early flights too.\n",
    "It also makes more sense because the model is not biased to think that all fights are delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter for arrived flights\n",
    "arrdf = df.where(~F.isnull(df.ARR_DELAY))\n",
    "arrdf.show(5)\n",
    "#print(arrdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and label\n",
    "arrdf = arrdf.select(\"YEAR\", \"MONTH\", \"DAY_OF_WEEK\", \"UNIQUE_CARRIER\", \\\n",
    "               \"DEST\", \"DEST_STATE_ABR\", \"CRS_DEP_TIME\",\\\n",
    "               \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"ARR_DELAY\")\n",
    "arrdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "print(arrdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take last 3 years of data to reduce size of dataset so it can be processes on one machine\n",
    "arrdf = arrdf.where(arrdf.YEAR > 2009)\n",
    "arrdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "print(arrdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features and label in files\n",
    "years = [2012-x for x in range (3)] # list of years in reverse.\n",
    "\n",
    "for year in years:\n",
    "        filename = \"FeaturesLabels\" + str(year) + \".csv\"\n",
    "        print(filename)\n",
    "        arrdf.where((arrdf.YEAR == year)).toPandas().to_csv(\"./Data/preprocR1-2/allArrivedFlights/\"+filename, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delays Greater than 15 with delay types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only delayed flights\n",
    "del15df = df.where(df.ARR_DEL15 > 0.5) \n",
    "del15df.show(5)\n",
    "#print(del15df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and label\n",
    "del15df = del15df.select(\"YEAR\", \"MONTH\", \"DAY_OF_WEEK\", \"UNIQUE_CARRIER\", \\\n",
    "               \"DEST\", \"DEST_STATE_ABR\", \"CRS_DEP_TIME\",\\\n",
    "               \"CRS_ELAPSED_TIME\", \"DISTANCE\",\"CARRIER_DELAY\", \\\n",
    "               \"WEATHER_DELAY\", \"NAS_DELAY\", \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\", \"ARR_DELAY\")\n",
    "del15df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "print(del15df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take data where the delay types are recorded (started recording at 6/2003)\n",
    "del15df = del15df.where(del15df.YEAR > 2003)\n",
    "del15df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "print(del15df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features and label in files\n",
    "years = [2012-x for x in range (9)] # list of years in reverse.\n",
    "\n",
    "for year in years:\n",
    "        filename = \"FeaturesLabels\" + str(year) + \".csv\"\n",
    "        print(filename)\n",
    "        del15df.where((del15df.YEAR == year)).toPandas().to_csv(\"./Data/preprocR1-2/delay15/\"+filename, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NULL values in original data\n",
    "df.select([F.count(when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check for NULL values and Nan values in delayed flights final data\n",
    "deldf.select([F.count(when(F.isnan(c), c)).alias(c) for c in deldf.columns]).show()\n",
    "deldf.select([F.count(when(F.isnull(c), c)).alias(c) for c in deldf.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NULL values and Nan values in all arrived flights final data\n",
    "arrdf.select([F.count(when(F.isnan(c), c)).alias(c) for c in arrdf.columns]).show()\n",
    "arrdf.select([F.count(when(F.isnull(c), c)).alias(c) for c in arrdf.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for NULL values and Nan values in all arrived flights final data\n",
    "#del15df.select([F.count(when(F.isnan(c), c)).alias(c) for c in del15df.columns]).show() #There are no nans as of last check\n",
    "del15df.select([F.count(when(F.isnull(c), c)).alias(c) for c in del15df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Your data at the moment\n",
    "data = sc.parallelize([ \n",
    "['Emily', 10, 1],\n",
    "['Word', None, 5],\n",
    "[None, 2, 6],\n",
    "['', None, 8]\n",
    "    ])\n",
    "\n",
    "# # Define schema\n",
    "# schema = StructType([\n",
    "#     StructField(\"Name\", StringType(), True),\n",
    "#     StructField(\"Height\", StringType(), True),\n",
    "#     StructField(\"Age\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# # Create dataframe\n",
    "# test = sqlContext.createDataFrame(data_converted, schema)\n",
    "\n",
    "test = sqlCtx.createDataFrame(data, ['name', 'height', 'age'])\n",
    "# Output\n",
    "test.show()\n",
    "#test.filter(F.count(F.isnull(test.height)).show()\n",
    "test.select([F.count(when(F.isnull(c), c)).alias(c) for c in test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
